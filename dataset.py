"""
This module generates datasets from a preprocessed data dictionary (e.g., a .pkl file).

The module supports converting a .pkl or .ppcs file into various dataset formats, such as graph datasets or]
based on the specified configurations.

- Input:
    original_dataset: A preprocessed data .pkl file.
    configfile: The config json file including the target dataset format information.

- Output:
    Datasets in different formats (e.g., vector dataset, graph dataset) as required.

- Configuration: The module reads from a config .json file, which allows users to specify
the columns to be used in the dataset generation. The config file must define the format and the
necessary columns required for each dataset type."""
import json
import os
import pickle
import datetime
import time
import argparse
from typing import Union, Optional, List, Tuple

import networkx as nx
from tqdm import tqdm

from scipy.sparse import coo_matrix, csr_matrix
from scipy.sparse.csgraph import connected_components
import pandas as pd
import numpy as np
import torch_geometric as pyg
import torch
from torch_geometric.config_store import Dataset
from torch_geometric.data import Data, InMemoryDataset, HeteroData
from torch_geometric.transforms import LargestConnectedComponents
import torch_geometric.transforms as T
from torch_geometric.utils import to_dense_adj, remove_self_loops, to_undirected
from pypower.api import case300
from pypower.idx_bus import ZONE
from scipy import sparse


def param_parser():
    """
    Return a parameter parser for generating a graph dataset from a preprocessed data dictionary.

    - Parameters:
        dataset_name: Name of the processed dataset.
        raw_data_file: Path to the .pkl file containing raw data (generated by data_preprocess.py).
        -r, --root: Root directory of the dataset. Default is 'data'.
        -c, --configfile: Path to the JSON config file containing dataset format information.
                          Default is 'src/graph_config.json'.

    - Returns:
        Argument parser with the defined parameters.
    """

    parser = argparse.ArgumentParser(description="Generates graph dataset "
                                                 "from a preprocessed data dictionary.")

    # Add required positional arguments
    parser.add_argument("-r", "--root", default=r"/mnt/data2/luliangyuchen/data/hz8000",
                        help="Root directory of the dataset. Default is 'data'.")
    parser.add_argument("--dataset_name", help="Name of the processed dataset",
                        default="Val-hz8000.pt")
    parser.add_argument("--num_splits", default=1, help="Number of the saved splits for large dataset")
    parser.add_argument("--type", choices=["ppcs", "pkl", "pt"], default="pt",
                        help="Type of the pre-processed dataset")

    parser.add_argument("--raw_data_file", help="Path to the source file containing "
                                                "the raw data from data_preprocess.py",
                        default=r"ieee-dataset/Vali-case39-N-1.ppcs")
    parser.add_argument("-c", "--configfile", default=None,
                        help="The config json file including the dataset format information. "
                             "The default is 'src\\graph_config_n_1.json'.")
    parser.add_argument("-s", "--sort", action="store_true", default=False,
                        help="If sort the data by Timestamp, default is False.")
    parser.add_argument("--add_global_node", action="store_true", default=False,
                        help="If add a global virtual node to the graph")
    parser.add_argument("--pos_encoding_dim", default=32,
                        help="Dim of the position encoding. Set to 0 if no pe.")

    return parser


def load_means_from_npz_by_size(path):
    z = np.load(path, allow_pickle=False)
    # 取 key 前缀集合（去掉后缀）
    prefixes = set(k.rsplit("_", 1)[0] for k in z.files if k.endswith("_count"))
    out = {}
    for key in sorted(prefixes):
        Bp = sparse.csr_matrix(
            (z[f"{key}_Bp_data"], z[f"{key}_Bp_indices"], z[f"{key}_Bp_indptr"]),
            shape=tuple(z[f"{key}_Bp_shape"]))
        Bpp = sparse.csr_matrix(
            (z[f"{key}_Bpp_data"], z[f"{key}_Bpp_indices"], z[f"{key}_Bpp_indptr"]),
            shape=tuple(z[f"{key}_Bpp_shape"]))
        c = int(z[f"{key}_count"][0])
        out[key] = {"Bp_mean": Bp, "Bpp_mean": Bpp, "count": c}
    return out


class GraphDataset(pyg.data.InMemoryDataset):

    def __init__(self,
                 root: str,
                 dataset_name: str,
                 transform=None,
                 pre_transform=None,
                 global_node=False,
                 pos_encoding_dim=0,
                 split_num=1,
                 bus_region_table=None,
                 bus_id_region_map:torch.Tensor=None,
                 ):
        self.dataset_name = dataset_name
        self.global_node = global_node
        self.pos_enc_dim = pos_encoding_dim
        self.bus_region_table = bus_region_table
        self.lookup = self._build_lookup_table(bus_id_region_map) if bus_id_region_map is not None else None
        self.LPE_memo = {}
        self.split_num = split_num
        super().__init__(root, transform, pre_transform)

        self.data, self.slices = torch.load(self.processed_paths[0], weights_only=False)

    @property
    def processed_file_names(self):
        return [f'{self.dataset_name}']

    def process(self):
        start_time = time.time()

        path = os.path.join(self.root, self.dataset_name)
        dataset = torch.load(path)
        print(f"Successfully loaded the dataset from: {path}")
        X = dataset["x"]
        Y = dataset["y"]
        Incidence = dataset["incidence"]
        Admittance = dataset["admittance"]
        J = dataset["j"]
        B_prime = dataset["b_prime"]
        tij = dataset["tij"]
        ###############
        br_incidence = dataset["br_incidence"]
        br_param = dataset["br_param"]
        sij = dataset["sij"]


        total = len(X)

        if self.split_num == 1:
            # 原始流程：单文件打包
            g_dataset = []
            for i in range(total):
                g_data = self._build_single_data(X[i], Y[i], Incidence[i], Admittance[i], J[i], sij[i], tij[i])
                g_dataset.append(g_data)

            data, slices = self.collate(g_dataset)
            torch.save((data, slices), self.processed_paths[0])
            print(f"[Single-file] Saved {total} samples to {self.processed_paths[0]}")

        else:
            # 分片保存
            part_len = (total + self.split_num - 1) // self.split_num
            for part_id in range(self.split_num):
                g_dataset = []
                start_idx = part_id * part_len
                end_idx = min((part_id + 1) * part_len, total)
                for i in range(start_idx, end_idx):
                    g_data = self._build_single_data(X[i], Y[i], Incidence[i], Admittance[i], J[i], sij[i], tij[i])
                    g_dataset.append(g_data)

                data, slices = self.collate(g_dataset)
                save_path = os.path.join(self.processed_dir, f'{self.dataset_name}_part_{part_id}.pt')
                torch.save((data, slices), save_path)
                print(f"[Split] Saved {len(g_dataset)} samples to {save_path}")

        end_time = time.time()
        print(
            f"Finished processing {total} samples in {self.split_num} part(s): {round(end_time - start_time, 2)} sec.")

    def _build_single_data(self, x, y, edge_index, admittance, J, Sij, Tij):
        edge_attr = torch.cat([admittance, J, Sij, Tij.unsqueeze(-1)], dim=-1)  # [G, B, J11, J12, J21, J22, Pij, Qij, Tij]
        g_data = Data(x=x, y=y, edge_index=edge_index, edge_attr=edge_attr)

        data_x_list = []
        data_x_list.append(g_data.x[:, :4])  # [p q v theta]
        if self.pos_enc_dim != 0:
            pos_enc = self.add_pos_encoding(g_data, pos_enc_dim=self.pos_enc_dim)
            data_x_list.append(pos_enc)  # pos_enc

        if self.bus_region_table is not None:  # 直接提供bus_region列 # region
            assert self.bus_region_table.shape[0] == g_data.x.shape[0]
            data_x_list.append(torch.tensor(self.bus_region_table).unsqueeze(-1))
        elif self.bus_id_region_map is not None:
            bus_region = self.lookup[g_data.x[:, -2].type(torch.long)].unsqueeze(-1)
            data_x_list.append(bus_region)

        data_x_list.append(g_data.x[:, -1].unsqueeze(-1))  # bus_type
        g_data.x = torch.cat(data_x_list, dim=-1)

        if self.pre_transform is not None:
            g_data = self.pre_transform(g_data)

        if self.global_node:
            g_data = self.add_global_node(g_data)

        return g_data

    def add_global_node(self, data):
        """
        在 Data 中增加一个“虚拟节点”，并与所有原节点双向连接。
        假设 data.x, data.edge_index, data.edge_attr 存在。
        """
        num_nodes = data.num_nodes
        d_feat = data.x.size(1)  # 原节点特征维度

        # 1. 虚拟节点特征 (默认全 0，可自定义)
        v_node_feat = data.x.new_zeros((1, d_feat))
        # 也可: torch.zeros(1, d_feat, dtype=data.x.dtype, device=data.x.device)

        # 拼接到 data.x
        data.x = torch.cat([data.x, v_node_feat], dim=0)
        v_node_id = num_nodes  # 虚拟节点 ID

        # 2. 增加边：虚拟节点 <-> 所有原节点 (双向)
        device = data.edge_index.device
        row_to = torch.full((num_nodes,), v_node_id, dtype=torch.long, device=device)
        col_to = torch.arange(num_nodes, device=device)
        v_to_others = torch.stack([row_to, col_to], dim=0)  # shape (2, num_nodes)

        row_from = torch.arange(num_nodes, device=device)
        col_from = torch.full((num_nodes,), v_node_id, dtype=torch.long, device=device)
        others_to_v = torch.stack([row_from, col_from], dim=0)  # shape (2, num_nodes)

        data.edge_index = torch.cat([data.edge_index, v_to_others, others_to_v], dim=1)

        # 3. 如果有 edge_attr，需要为新增边初始化属性
        if data.edge_attr is not None:
            d_attr = data.edge_attr.size(1)
            new_edges_attr = data.edge_attr.new_zeros((2 * num_nodes, d_attr))
            data.edge_attr = torch.cat([data.edge_attr, new_edges_attr], dim=0)

        if data.y is not None:
            d_y = data.y.size(1)
            new_y = data.y.new_zeros((1, d_y))
            data.y = torch.cat([data.y, new_y], dim=0)

        return data

    def add_pos_encoding(self, data, pos_enc_dim):

        adj = to_dense_adj(data.edge_index).squeeze(0)

        # 获取当前 batch 中第 i 个图的子图
        num_nodes = data.num_nodes

        if num_nodes == 1:  # 只有一个节点，LPE 设为零向量
            return torch.zeros((1, pos_enc_dim))

        key = (num_nodes, pos_enc_dim, adj.cpu().numpy().tobytes())

        # 如果该拓扑的 LPE 已经计算过，则直接使用缓存结果
        if key in self.LPE_memo:
            pos_enc = self.LPE_memo[key]
        else:
            deg = adj.sum(dim=1)
            # 构造 D^{-1/2} 对角矩阵，避免除零，加上一个微小值 1e-6
            D_inv_sqrt = torch.diag(1.0 / torch.sqrt(deg + 1e-6))

            # 计算归一化的拉普拉斯矩阵 L = I - D^{-1/2} A D^{-1/2}
            L = torch.eye(num_nodes, device=adj.device) - D_inv_sqrt @ adj @ D_inv_sqrt

            # 对拉普拉斯矩阵做特征分解，取前 pos_enc_dim 个特征向量
            eigvals, eigvecs = torch.linalg.eigh(L)
            pos_enc = eigvecs[:, :pos_enc_dim]

            # 将计算结果存入缓存中，下次遇到相同拓扑可直接使用
            self.LPE_memo[key] = pos_enc

        return pos_enc

    def _build_lookup_table(self, mapping: torch.Tensor):
        max_key = mapping[:, 0].max().item() + 1
        lookup = torch.full((max_key,), -1, dtype=mapping.dtype)
        lookup[mapping[:, 0]] = mapping[:, 1]
        return lookup

class HeteroDataset(pyg.data.InMemoryDataset):

    def __init__(self,
                 root: str,
                 dataset_name: str,
                 transform=None,
                 pre_transform=None,
                 pos_encoding_dim=0,
                 bus_region_table=None,
                 bus_id_region_map:torch.Tensor=None,
                 Bp_inv=None,
                 ):
        self.dataset_name = dataset_name
        self.pos_enc_dim = pos_encoding_dim
        self.bus_region_table = bus_region_table
        self.lookup = self._build_lookup_table(bus_id_region_map) if bus_id_region_map is not None else None
        self.LPE_memo = {}
        self.Bp_inv = Bp_inv
        super().__init__(root, transform, pre_transform)

        self.data, self.slices = torch.load(self.processed_paths[0], weights_only=False)

    def _build_lookup_table(self, mapping: torch.Tensor):
        max_key = mapping[:, 0].max().item() + 1
        lookup = torch.full((max_key,), -1, dtype=mapping.dtype)
        lookup[mapping[:, 0]] = mapping[:, 1]
        return lookup

    @property
    def processed_file_names(self):
        return [f'{self.dataset_name}']

    def process(self):
        start_time = time.time()

        path = os.path.join(self.root, self.dataset_name)
        dataset = torch.load(path, weights_only=False)
        print(f"Successfully loaded the dataset from: {path}")
        X = dataset["x"]
        Y = dataset["y"]
        Incidence = dataset["incidence"]
        Admittance = dataset["admittance"]
        J = dataset["j"]
        B_prime = dataset.get("b_prime", None)
        tij = dataset.get("tij", None)
        ###############
        br_incidence = dataset.get("br_incidence", None)
        br_param = dataset.get("br_param", None)
        sij = dataset.get("sij", None)

        total = len(X)
        g_dataset = []
        for i in tqdm(range(total)):
            t = tij[i] if tij is not None else None
            br_inci = br_incidence[i] if br_incidence is not None else None
            br_par = br_param[i] if br_param is not None else None
            br_s = sij[i] if sij is not None else None
            g_data = self._build_single_data(X[i], Y[i], Incidence[i], Admittance[i], J[i], B_prime[i], t,
                                             br_inci, br_par, br_s)
            g_dataset.append(g_data)

        data, slices = self.collate(g_dataset)
        torch.save((data, slices), self.processed_paths[0])
        print(f"[Single-file] Saved {total} samples to {self.processed_paths[0]}")

        end_time = time.time()
        print(
            f"Finished processing {total} samples: {round(end_time - start_time, 2)} sec.")

    def _build_single_data(self, x, y,
                           sp_incidence, admittance, J, Bp, Tij,
                           br_incidence, br_param, Sij):
        """
        x:  (N, F)  -> [p, q, v, theta, ..., bus_type]
        y:  (N, 4)  -> [v, theta, p, q]  (监督)
        sp_incidence: (2, E_sp) LongTensor
        admittance, J, Bp, Tij: 按最后一维拼接
        br_incidence: (2, E_br) LongTensor
        br_param: (E_br, 5) -> [r, x, b, tap, shift]
        Sij: (E_br, 4) -> [Pf, Qf, Pt, Qt] (p.u.)
        """
        # 统一 device/dtype
        device = x.device if isinstance(x, torch.Tensor) else torch.device('cpu')
        to_f64 = lambda t: t.to(device=device, dtype=torch.float64)
        to_i64 = lambda t: t.to(device=device, dtype=torch.long)

        g_data = HeteroData()
        N = y.shape[0]
        g_data['bus'].num_nodes = int(N)

        # ---------- 节点监督 ----------
        g_data['bus'].y = to_f64(y)  # [v, theta, p, q]

        # ---------- 节点特征 ----------
        feats = []
        x = to_f64(x)
        feats.append(x[:, :4])  # [p, q, v, theta]

        if self.pos_enc_dim and self.pos_enc_dim > 0:
            pos_enc = self.add_pos_encoding(
                g_data,
                edge_index_sp=to_i64(sp_incidence),
                edge_index_br=to_i64(br_incidence) if br_incidence is not None else None,
                pos_enc_dim=self.pos_enc_dim
            )
            feats.append(pos_enc)  # LPE

        if self.bus_region_table is not None:
            region = torch.as_tensor(self.bus_region_table, device=device)
            if region.dim() == 1:
                region = region.unsqueeze(-1)
            assert region.shape[0] == N, f"bus_region_table rows {region.shape[0]} != N {N}"
            feats.append(to_f64(region))
        elif self.lookup is not None:
            bus_region = self.lookup[x[:, -2].type(torch.long)].unsqueeze(-1)
            feats.append(to_f64(bus_region))  # bus_region_id
        else:
            feats.append(to_f64(x[:, -2].unsqueeze(-1)))  # bus_id

        feats.append(x[:, -1].unsqueeze(-1))  # bus_type

        g_data['bus'].x = torch.cat(feats, dim=-1)  # (N, F_total)

        # ---------- bus - sp - bus ----------
        g_data['bus', 'sp', 'bus'].edge_index = to_i64(sp_incidence)
        # 确保所有 edge_attr 维度一致按最后一维拼接
        edge_attr_sp = torch.cat([to_f64(admittance),
                                  to_f64(J),
                                  to_f64(Bp).unsqueeze(-1)], dim=-1)
        if Tij is not None:
            edge_attr_sp = torch.cat([edge_attr_sp, to_f64(Tij).unsqueeze(-1)], dim=-1)
        g_data['bus', 'sp', 'bus'].edge_attr = edge_attr_sp

        # ---------- bus - br - bus ----------
        if br_incidence is not None:
            g_data['bus', 'br', 'bus'].edge_index = to_i64(br_incidence)
            edge_attr_br = torch.cat([to_f64(torch.tensor(br_param)), to_f64(Sij)], dim=-1)

            # 约定为 [r, x, b, tap, shift, Pf, Qf, Pt, Qt]
            g_data['bus', 'br', 'bus'].edge_attr = edge_attr_br

        # 提取最大连通图
        # g_data_sci, idx_keep, _ = hetero_lcc_scipy(g_data)
        g_data = hetero_lcc_nx(g_data, ref_edge_type=('bus', 'sp', 'bus'))

        if self.pre_transform is not None:
            g_data = self.pre_transform(g_data)

        return g_data

    def add_pos_encoding(self, data: HeteroData, pos_enc_dim: int,
                         relation: str = 'sp',
                         edge_index_sp: torch.Tensor = None,
                         edge_index_br: torch.Tensor = None):
        """
        基于选定关系的 edge_index 计算 Laplacian 位置编码。
        - relation: 'sp' 或 'br'
        - 返回: (N, pos_enc_dim)
        缓存 key 使用 (N, pos_enc_dim, relation, edge_index_bytes)
        """
        N = data['bus'].num_nodes
        device = edge_index_sp.device if edge_index_sp is not None else torch.device('cpu')

        if N == 1 or pos_enc_dim <= 0:
            return torch.zeros((N, pos_enc_dim), device=device, dtype=torch.float32)

        if relation == 'sp':
            edge_index = edge_index_sp
        elif relation == 'br':
            edge_index = edge_index_br
        else:
            raise ValueError("relation must be 'sp' or 'br'")

        # 缓存 key：边索引 bytes（比 dense 邻接更小也可区分拓扑）
        key = (int(N), int(pos_enc_dim), relation, edge_index.detach().cpu().numpy().tobytes())
        if key in self.LPE_memo:
            return self.LPE_memo[key].to(device)

        # 构造邻接（无权；若要带权可自行改 to_dense_adj 的 edge_attr）
        A = to_dense_adj(edge_index, max_num_nodes=N).squeeze(0)  # (N,N), float
        # 度与规约拉普拉斯
        deg = A.sum(dim=1)
        D_inv_sqrt = torch.diag(1.0 / torch.sqrt(deg + 1e-6))
        L = torch.eye(N, device=A.device, dtype=A.dtype) - D_inv_sqrt @ A @ D_inv_sqrt

        # 特征分解（取最小的 k 个特征向量）：eigh 返回升序
        evals, evecs = torch.linalg.eigh(L)  # (N,), (N,N)
        pos_enc = evecs[:, :pos_enc_dim].to(torch.float32)

        self.LPE_memo[key] = pos_enc.detach().cpu()
        return pos_enc.to(device)




def hetero_lcc_scipy(
    data_hetero: HeteroData,
    ref_edge_type: Tuple[str, str, str] = ('bus', 'sp', 'bus'),
    *,
    directed: bool = False,                # False=无向；True=有向（下面 connection='weak'）
    use_abs: bool = False,                 # True：按 |w| 做阈值过滤；False：按 w
    symmetrize: bool = True,               # 是否无向化：A <- A ∪ A^T（建议 True，除非你要强连通/方向性）
    threshold: float = 0.0,                # 过滤 |w| > threshold 的边
    weight_key: Optional[str] = None,      # 边权所在的字段名，例如 'weight' 或 'edge_weight'；None=全1
    return_all_labels: bool = False,       # 是否额外返回所有节点的分量标签
) -> Tuple[HeteroData, torch.Tensor, Optional[np.ndarray]]:
    """
    基于指定边类型 ref_edge_type 判定连通性，提取最大连通子图（LCC），并在原始 HeteroData 上做诱导子图。

    返回：
      sub      : HeteroData，LCC 诱导子图（保留所有边类型在该节点集上的子集与其特征）
      idx_keep : torch.LongTensor，属于 LCC 的原始节点索引（升序）
      labels   : np.ndarray[int]（可选），每个节点的分量编号（0..n_comp-1）
    """
    # ---- 0) 断言与元信息 ----
    node_types = list(data_hetero.node_types)
    edge_types = list(data_hetero.edge_types)
    assert len(node_types) == 1, f"要求只有 1 种节点类型，实际 {len(node_types)}: {node_types}"
    ntype = node_types[0]
    assert ref_edge_type in edge_types, f"{ref_edge_type} 不在边类型中：{edge_types}"

    N = int(data_hetero[ntype].num_nodes)
    if N == 0:
        # 空图：直接按空返回
        empty = HeteroData()
        empty[ntype].num_nodes = 0
        for et in edge_types:
            empty[et].edge_index = torch.empty((2, 0), dtype=torch.long)
            for k, v in data_hetero[et].items():
                if k == 'edge_index':
                    continue
                empty[et][k] = v.new_empty((0,) + tuple(v.shape[1:]))
        for k, v in data_hetero[ntype].items():
            if k == 'num_nodes':
                continue
            empty[ntype][k] = v.new_empty((0,) + tuple(v.shape[1:]))
        return empty, torch.empty(0, dtype=torch.long), (np.array([], dtype=int) if return_all_labels else None)

    # ---- 1) 拿参考边的 edge_index (+可选边权) 并构建 SciPy 邻接矩阵 ----
    ei = data_hetero[ref_edge_type].edge_index
    if ei.numel() == 0:
        # 没有参考边 => 认为没有连通的边；这里返回空诱导子图（也可选择保留全部节点）
        empty = HeteroData()
        empty[ntype].num_nodes = 0
        for et in edge_types:
            empty[et].edge_index = torch.empty((2, 0), dtype=torch.long)
            for k, v in data_hetero[et].items():
                if k == 'edge_index':
                    continue
                empty[et][k] = v.new_empty((0,) + tuple(v.shape[1:]))
        for k, v in data_hetero[ntype].items():
            if k == 'num_nodes':
                continue
            empty[ntype][k] = v.new_empty((0,) + tuple(v.shape[1:]))
        return empty, torch.empty(0, dtype=torch.long), (np.zeros(N, dtype=int) if return_all_labels else None)

    rows = ei[0].detach().cpu().numpy().astype(np.int64, copy=False)
    cols = ei[1].detach().cpu().numpy().astype(np.int64, copy=False)

    if weight_key is not None and (weight_key in data_hetero[ref_edge_type]):
        w = data_hetero[ref_edge_type][weight_key]
        w_np = w.detach().cpu().numpy()
    else:
        w_np = np.ones(rows.shape[0], dtype=np.float64)

    # 去自环（不影响连通分量，但能避免噪声）
    mask_not_loop = rows != cols
    rows = rows[mask_not_loop]
    cols = cols[mask_not_loop]
    w_np = w_np[mask_not_loop]

    # 阈值过滤
    if threshold > 0.0:
        if use_abs:
            m = np.abs(w_np) > threshold
        else:
            m = w_np > threshold
        rows, cols, w_np = rows[m], cols[m], w_np[m]

    # 构建 COO，再转 CSR
    A = coo_matrix((w_np, (rows, cols)), shape=(N, N), dtype=w_np.dtype).tocsr()
    # 无向化（若需要）
    if symmetrize:
        A_und = A.maximum(A.T)
    else:
        A_und = A

    # 对角清零，去掉可能的孤立自环
    A_und.setdiag(0)
    A_und.eliminate_zeros()

    # ---- 2) 连通分量（scipy 的 connected_components）----
    # directed=False + connection='weak' => 无向连通
    # directed=True  + connection='weak' => 有向图的弱连通
    n_comp, labels = connected_components(
        A_und, directed=directed, connection='weak', return_labels=True
    )

    if n_comp == 0:
        # 全空
        idx_keep = torch.empty(0, dtype=torch.long)
        sub = data_hetero.subgraph({ntype: idx_keep})
        return sub, idx_keep, (labels if return_all_labels else None)

    # 找最大分量
    sizes = np.bincount(labels, minlength=n_comp)
    comp_id = int(sizes.argmax())
    idx_np = np.flatnonzero(labels == comp_id)       # 升序
    idx_keep = torch.from_numpy(idx_np.astype(np.int64, copy=False))

    # ---- 3) 回到 HeteroData 做诱导子图（会自动筛所有边类型 + 特征）----
    sub = data_hetero.subgraph({ntype: idx_keep})

    if return_all_labels:
        return sub, idx_keep, labels
    return sub, idx_keep, None

def hetero_lcc_nx(
    data_hetero: HeteroData,
    ref_edge_type: Tuple[str, str, str] = ('bus', 'sp', 'bus'),
    num_components: int = 1,
    connection: str = 'weak',           # 'weak'（无向连通）或 'strong'（强连通，需有向）
    remove_selfloops: bool = True,
) -> HeteroData:
    """
    用 NetworkX 基于指定边类型 ref_edge_type 计算连通分量（默认 weak / 无向），
    取最大的前 K 个分量的节点集合，然后在原始 HeteroData 上做诱导子图：
      - 节点特征：保留这部分节点对应的行
      - 边与边特征：保留所有边类型中两端都在该节点集合内的边及其特征

    约束：
      - 只有一种节点类型（assert）
      - ref_edge_type 必须存在
    """
    # 0) 断言与基本信息
    node_types: List[str] = list(data_hetero.node_types)
    edge_types: List[Tuple[str, str, str]] = list(data_hetero.edge_types)
    assert len(node_types) == 1, f"要求只有一种节点类型，实际 {len(node_types)}: {node_types}"
    ntype = node_types[0]
    assert ref_edge_type in edge_types, f"{ref_edge_type} 不在边类型中：{edge_types}"

    num_nodes: int = int(data_hetero[ntype].num_nodes)
    ei = data_hetero[ref_edge_type].edge_index  # [2, E]

    # 1) 构建 NetworkX 图（按需要选择无向/有向）
    if connection.lower() == 'weak':
        G = nx.Graph()
    elif connection.lower() == 'strong':
        G = nx.DiGraph()
    else:
        raise ValueError("connection 仅支持 'weak' 或 'strong'")

    G.add_nodes_from(range(num_nodes))

    # 2) 加入边（可选移除自环）
    if ei.numel() > 0:
        src = ei[0].tolist()
        dst = ei[1].tolist()
        if remove_selfloops:
            edges = [(u, v) for u, v in zip(src, dst) if u != v]
        else:
            edges = list(zip(src, dst))
        # weak 连通使用无向图；strong 使用有向图
        # 如果是 weak，但你的 ref_edge 本身是有向的，也可以先加有向再转无向；
        # 这里直接用无向/有向容器控制
        G.add_edges_from(edges)

    # 3) 计算连通分量
    if connection.lower() == 'weak':
        comps = list(nx.connected_components(G))  # 每个元素是一个节点集合 set[int]
    else:  # strong
        comps = list(nx.strongly_connected_components(G))

    if len(comps) == 0:
        # 没有边：按单节点分量处理；或直接返回空
        # 这里选择返回空诱导子图（也可改成保留所有节点）
        empty = HeteroData()
        empty[ntype].num_nodes = 0
        for et in edge_types:
            empty[et].edge_index = torch.empty((2, 0), dtype=torch.long)
            for key, val in data_hetero[et].items():
                if key == 'edge_index':
                    continue
                empty[et][key] = val.new_empty((0,) + tuple(val.shape[1:]))
        for key, val in data_hetero[ntype].items():
            if key == 'num_nodes':
                continue
            empty[ntype][key] = val.new_empty((0,) + tuple(val.shape[1:]))
        return empty

    # 4) 选前 K 个最大分量
    comps.sort(key=len, reverse=True)
    k = max(1, min(num_components, len(comps)))
    keep_set = set().union(*comps[:k])

    if not keep_set:
        # 与上面一致，返回空图
        empty = HeteroData()
        empty[ntype].num_nodes = 0
        for et in edge_types:
            empty[et].edge_index = torch.empty((2, 0), dtype=torch.long)
            for key, val in data_hetero[et].items():
                if key == 'edge_index':
                    continue
                empty[et][key] = val.new_empty((0,) + tuple(val.shape[1:]))
        for key, val in data_hetero[ntype].items():
            if key == 'num_nodes':
                continue
            empty[ntype][key] = val.new_empty((0,) + tuple(val.shape[1:]))
        return empty

    keep_nodes = torch.tensor(sorted(keep_set), dtype=torch.long)

    # 5) 在原始异构图上做诱导子图（自动筛边 + 重映射 + 同步特征）
    sub = data_hetero.subgraph({ntype: keep_nodes})
    return sub

def load_config_file(path):
    """Load a config .json file"""
    try:
        with open(path, "r", encoding="utf-8") as f:
            configfile = json.load(f)
        return configfile
    except FileExistsError as e:
        raise FileExistsError(f"Config file {configfile} not found.") from e


def main():
    """Main Entrance of the module"""

    parser = param_parser()
    args = parser.parse_args()

    if args.type == 'pt':
        # case = case300()
        # bus_region_table = case["bus"][:, ZONE]
        # bus_region_table[bus_region_table == 9.0] = 4.0

        # bus id region map for hz8000
        bus_id_region_map = torch.load('hz8000_bus_id_region_map_40.pt')
        dataset_names = [f'Test-hz8000.pt', 'Val-hz8000.pt']
        for dataset_name in dataset_names:
            HeteroDataset(root=args.root, dataset_name=dataset_name,
                          pos_encoding_dim=args.pos_encoding_dim, bus_region_table=None,
                          bus_id_region_map=bus_id_region_map)

if __name__ == "__main__":
    main()
